{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb20c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import time, pickle, math, warnings, os, operator\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import xgboost\n",
    "\n",
    "data_nova = pd.read_csv('./dataset/fileLevel/Nova.csv', dtype=None, sep=',').to_numpy()\n",
    "data_ironic = pd.read_csv('./dataset/fileLevel/Ironic.csv', dtype=None, sep=',').to_numpy()\n",
    "data_base = pd.read_csv('./dataset/fileLevel/Base.csv', dtype=None, sep=',').to_numpy()\n",
    "\n",
    "model_path = './dataset/ml-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2395869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetFromRawData(project_source, data, bias):\n",
    "    row_data = data[0:,3]\n",
    "    row_data_Y = data[0:,0]\n",
    "    row_data_deletions = data[0:,6]\n",
    "    row_data_additions = data[0:,7]\n",
    "    row_data_changedLine = data[0:,8]\n",
    "  \n",
    "    Y_train = []\n",
    "    is_comment = 0\n",
    "    not_comment = 0\n",
    "    for element in row_data_Y:\n",
    "        if(element == 0):\n",
    "            Y_train.append(False)\n",
    "            not_comment += 1\n",
    "        else:\n",
    "            Y_train.append(True)\n",
    "            is_comment += 1\n",
    "    Y_train = np.array(Y_train)\n",
    "\n",
    "    first = int(len(data)*0.6)\n",
    "    # second = int(len(data)*0.2) + first + 49\n",
    "    divider = int(len(data)*0.2) + first + bias\n",
    "#     second = int(len(data)*0.2) + first + 2\n",
    "    # second = int(len(data)*0.7)#6:4\n",
    "\n",
    "    # data_count_vect = CountVectorizer(lowercase=False)\n",
    "    # data_count_vect = CountVectorizer(lowercase=False)\n",
    "    data_count_vect = CountVectorizer(min_df=2, max_df=0.5)\n",
    "    # data_count_vect = CountVectorizer(min_df=3, max_df=0.5)\n",
    "\n",
    "    train_row_data = row_data[:divider]\n",
    "    test_row_data = row_data[divider:]\n",
    "\n",
    "    train_row_data_deletions = row_data_deletions[:divider]\n",
    "    test_row_data_deletions = row_data_deletions[divider:]\n",
    "\n",
    "    train_row_data_additions = row_data_additions[:divider]\n",
    "    test_row_data_additions = row_data_additions[divider:]\n",
    "\n",
    "    train_row_data_changedLine = row_data_changedLine[:divider]\n",
    "    test_row_data_changedLine = row_data_changedLine[divider:]\n",
    "\n",
    "    data_train_counts = data_count_vect.fit_transform(train_row_data)\n",
    "    data_test_counts = data_count_vect.transform(test_row_data)\n",
    "\n",
    "    final_train_X = np.hstack((data_train_counts.toarray(),train_row_data_deletions[:,None]))\n",
    "    final_train_X = np.hstack((final_train_X,train_row_data_additions[:,None]))\n",
    "    final_train_X = np.hstack((final_train_X,train_row_data_changedLine[:,None]))\n",
    "\n",
    "    final_test_X = np.hstack((data_test_counts.toarray(),test_row_data_deletions[:,None]))\n",
    "    final_test_X = np.hstack((final_test_X,test_row_data_additions[:,None]))\n",
    "    final_test_X = np.hstack((final_test_X,test_row_data_changedLine[:,None]))\n",
    "    final_train_y = Y_train[:divider]\n",
    "    final_test_y = Y_train[divider:]\n",
    "    print(data[:divider][-1])\n",
    "    print(data[divider:][0])\n",
    "    del data_train_counts,data_test_counts,train_row_data,test_row_data,data,row_data,row_data_Y \n",
    "    return final_train_X,final_train_y,final_test_X,final_test_y,divider,data_count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284d73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResult(x, y, model):\n",
    "    print(roc_auc_score(y, model.predict_proba(x)[:,1]))\n",
    "    print(precision_score(y, model.predict(x)))\n",
    "    print(recall_score(y, model.predict(x)))\n",
    "    print(f1_score(y, model.predict(x)))\n",
    "    print(matthews_corrcoef(y, model.predict(x)))\n",
    "    print(confusion_matrix(y, model.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c8aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRFmodel(project,rf_train_X,rf_train_y,rf_test_X,rf_test_y,seed):\n",
    "    print(\"RF seed:\" + str(seed))\n",
    "    train_rf_model_path = model_path+'/RQ2_2_rf_'+project+'-'+str(seed)+'.pkl'\n",
    "    if not os.path.exists(train_rf_model_path):\n",
    "        rf = RandomForestClassifier(n_estimators=200,n_jobs=-1,random_state=seed)\n",
    "        rf_X, rf_y = SMOTE(k_neighbors=10, random_state=seed).fit_resample(rf_train_X, rf_train_y)\n",
    "        rf.fit(rf_X,rf_y)\n",
    "        rf_ouput = open(train_rf_model_path, 'wb')\n",
    "        pickle.dump(rf,rf_ouput)\n",
    "        print(\"finish to creat a new model\")\n",
    "    else:\n",
    "        with open(train_rf_model_path,'rb') as f:\n",
    "            rf = pickle.load(f)\n",
    "    printResult(rf_test_X,rf_test_y,rf)\n",
    "    return rf\n",
    "\n",
    "# TN FP\n",
    "# FN TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f7bbec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic\n",
    "def trainLGmodel(project,train_X,train_y,test_X,test_y,seed):\n",
    "    print(\"LG seed:\" + str(seed))\n",
    "    train_lg_model_path = model_path+'/RQ2_2_lg_'+project+'-'+str(seed)+'.pkl'\n",
    "    if not os.path.exists(train_lg_model_path):\n",
    "        lg = linear_model.LogisticRegression(penalty='l2', C=1, solver = 'newton-cg', random_state=seed)\n",
    "        lg.fit(train_X,train_y)\n",
    "        lg_ouput = open(train_lg_model_path, 'wb')\n",
    "        pickle.dump(lg,lg_ouput)\n",
    "        print(\"finish to creat a new lg model\")\n",
    "    else:\n",
    "        with open(train_lg_model_path,'rb') as f:\n",
    "            lg = pickle.load(f)\n",
    "    printResult(test_X,test_y,lg)\n",
    "    return lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8edaef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB\n",
    "def trainNBmodel(project,train_X,train_y,test_X,test_y,seed):\n",
    "    print(\"NB seed:\" + str(seed))\n",
    "    train_nb_model_path = model_path+'/RQ2_2_nb_'+project+'-'+str(seed)+'.pkl'\n",
    "    if not os.path.exists(train_nb_model_path):\n",
    "        nb = MultinomialNB()\n",
    "        nb.fit(train_X,train_y)\n",
    "        nb_ouput = open(train_nb_model_path, 'wb')\n",
    "        pickle.dump(nb,nb_ouput)\n",
    "        print(\"finish to creat a new nb model\")\n",
    "    else:\n",
    "        with open(train_nb_model_path,'rb') as f:\n",
    "            nb = pickle.load(f)\n",
    "    printResult(test_X,test_y,nb)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "899501a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT\n",
    "def trainDTmodel(project,train_X,train_y,test_X,test_y,seed):\n",
    "    print(\"DT seed:\" + str(seed))\n",
    "    train_dt_model_path = model_path+'/RQ2_2_dt_'+project+'-'+str(seed)+'.pkl'\n",
    "    if not os.path.exists(train_dt_model_path):\n",
    "        dt = DecisionTreeClassifier(random_state=seed)\n",
    "        dt.fit(train_X,train_y)\n",
    "        dt_ouput = open(train_dt_model_path, 'wb')\n",
    "        pickle.dump(dt,dt_ouput)\n",
    "        print(\"finish to creat a new dt model\")\n",
    "    else:\n",
    "        with open(train_dt_model_path,'rb') as f:\n",
    "            dt = pickle.load(f)\n",
    "    printResult(test_X,test_y,dt)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec931a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB\n",
    "def trainXGBmodel(project,train_X,train_y,test_X,test_y,seed):\n",
    "    print(\"Xgb seed:\" + str(seed))\n",
    "    train_xgb_model_path = model_path+'/RQ2_2_xgb_'+project+'-'+str(seed)+'.pkl'\n",
    "    if not os.path.exists(train_xgb_model_path):\n",
    "        xgb = xgboost.XGBClassifier(nestimators=200,random_state=seed)\n",
    "        xgb.fit(train_X, train_y)\n",
    "        xgb_ouput = open(train_xgb_model_path, 'wb')\n",
    "        pickle.dump(xgb,xgb_ouput)\n",
    "        print(\"finish to creat a new xgb model\")\n",
    "    else:\n",
    "        with open(train_xgb_model_path,'rb') as f:\n",
    "            xgb = pickle.load(f)\n",
    "    printResult(test_X,test_y,xgb)\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc07b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB\n",
    "def trainDMmodel(project,train_X,train_y,test_X,test_y,seed):\n",
    "    print(\"dummy seed:\" + str(seed))\n",
    "    train_dm_model_path = model_path+'/RQ2_2_dm_'+project+'-'+str(seed)+'.pkl'\n",
    "    if not os.path.exists(train_dm_model_path):\n",
    "        dm = DummyClassifier(strategy='stratified',random_state=seed)\n",
    "        dm.fit(train_X, train_y)\n",
    "        dm_ouput = open(train_dm_model_path, 'wb')\n",
    "        pickle.dump(dm,dm_ouput)\n",
    "        print(\"finish to creat a new dm model\")\n",
    "    else:\n",
    "        with open(train_dm_model_path,'rb') as f:\n",
    "            dm = pickle.load(f)\n",
    "    printResult(test_X,test_y,dm)\n",
    "    return dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "582411dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataStatastic(train_X, train_y, test_X, test_y,data_count_vect):\n",
    "    train_true_label = 0\n",
    "    test_true_label = 0\n",
    "    for label in train_y:\n",
    "        if label == 1:\n",
    "            train_true_label += 1\n",
    "    for label in test_y:\n",
    "        if label == 1:\n",
    "            test_true_label += 1\n",
    "    print(\"training dataset size:\", len(train_X))\n",
    "    print(\"training dataset positive label size:\", train_true_label, train_true_label/len(train_y))\n",
    "    print(\"test dataset size:\", len(test_X))\n",
    "    print(\"test dataset positive label size:\", test_true_label, test_true_label/len(test_X))\n",
    "    print(\"feature_size:\", len(data_count_vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22b959a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResult(project_source,seed,projectName,data,bias):\n",
    "    projectName = projectName\n",
    "    #RF\n",
    "    train_X, train_y, test_X, test_y,divider,data_count_vect = getDatasetFromRawData(project_source,data, bias)\n",
    "    dataStatastic(train_X, train_y, test_X, test_y,data_count_vect)\n",
    "    rf = trainRFmodel(projectName,train_X, train_y, test_X, test_y,seed)\n",
    "    lg = trainLGmodel(projectName,train_X, train_y, test_X, test_y,seed)\n",
    "    nb = trainNBmodel(projectName,train_X, train_y, test_X, test_y,seed)\n",
    "    dt = trainDTmodel(projectName,train_X, train_y, test_X, test_y,seed)\n",
    "    xgb = trainXGBmodel(projectName,train_X, train_y, test_X, test_y,seed)\n",
    "    dm = trainDMmodel(projectName,train_X, train_y, test_X, test_y,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e30ad777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: 9602\n",
      "training dataset positive label size: 3111 0.3239950010414497\n",
      "test dataset size: 2395\n",
      "test dataset positive label size: 448 0.18705636743215032\n",
      "feature_size: 6393\n",
      "RF seed:2\n",
      "finish to creat a new model\n",
      "0.7674421270819576\n",
      "0.5156695156695157\n",
      "0.40401785714285715\n",
      "0.4530663329161451\n",
      "0.3492059087776338\n",
      "[[1777  170]\n",
      " [ 267  181]]\n",
      "LG seed:2\n",
      "finish to creat a new lg model\n",
      "0.6242817475603493\n",
      "0.36674816625916873\n",
      "0.33482142857142855\n",
      "0.35005834305717615\n",
      "0.20911434367467477\n",
      "[[1688  259]\n",
      " [ 298  150]]\n",
      "NB seed:2\n",
      "finish to creat a new nb model\n",
      "0.6057929094210874\n",
      "0.33562822719449226\n",
      "0.43526785714285715\n",
      "0.3790087463556851\n",
      "0.21562023723631404\n",
      "[[1561  386]\n",
      " [ 253  195]]\n",
      "DT seed:2\n",
      "finish to creat a new dt model\n",
      "0.584426475713552\n",
      "0.33098591549295775\n",
      "0.31473214285714285\n",
      "0.32265446224256294\n",
      "0.17167831345157145\n",
      "[[1662  285]\n",
      " [ 307  141]]\n",
      "Xgb seed:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:46:19] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"nestimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:46:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "finish to creat a new xgb model\n",
      "0.755815953114682\n",
      "0.4349315068493151\n",
      "0.28348214285714285\n",
      "0.34324324324324323\n",
      "0.2368581298153358\n",
      "[[1782  165]\n",
      " [ 321  127]]\n",
      "dummy seed:2\n",
      "finish to creat a new dm model\n",
      "0.4894652487343165\n",
      "0.1766304347826087\n",
      "0.29017857142857145\n",
      "0.2195945945945946\n",
      "-0.017807959703329\n",
      "[[1341  606]\n",
      " [ 318  130]]\n"
     ]
    }
   ],
   "source": [
    "getResult(\"openstack\",2,\"nova\",data_nova,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e478cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: 3349\n",
      "training dataset positive label size: 882 0.26336219767094654\n",
      "test dataset size: 844\n",
      "test dataset positive label size: 199 0.235781990521327\n",
      "feature_size: 3373\n",
      "RF seed:2\n",
      "finish to creat a new model\n",
      "0.6779478789295313\n",
      "0.3813953488372093\n",
      "0.4120603015075377\n",
      "0.39613526570048313\n",
      "0.2005539299285651\n",
      "[[512 133]\n",
      " [117  82]]\n",
      "LG seed:2\n",
      "finish to creat a new lg model\n",
      "0.5928362743952319\n",
      "0.35454545454545455\n",
      "0.39195979899497485\n",
      "0.3723150357995227\n",
      "0.16612613190273592\n",
      "[[503 142]\n",
      " [121  78]]\n",
      "NB seed:2\n",
      "finish to creat a new nb model\n",
      "0.6325464531962136\n",
      "0.31266846361185985\n",
      "0.5829145728643216\n",
      "0.4070175438596491\n",
      "0.16041403944894447\n",
      "[[390 255]\n",
      " [ 83 116]]\n",
      "DT seed:2\n",
      "finish to creat a new dt model\n",
      "0.5373923882980796\n",
      "0.2777777777777778\n",
      "0.3768844221105528\n",
      "0.3198294243070362\n",
      "0.0678527906416606\n",
      "[[450 195]\n",
      " [124  75]]\n",
      "Xgb seed:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:46:42] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"nestimators\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:46:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "finish to creat a new xgb model\n",
      "0.6320634178645164\n",
      "0.38372093023255816\n",
      "0.3316582914572864\n",
      "0.3557951482479785\n",
      "0.17631860445592892\n",
      "[[539 106]\n",
      " [133  66]]\n",
      "dummy seed:2\n",
      "finish to creat a new dm model\n",
      "0.49328425071091897\n",
      "0.22596153846153846\n",
      "0.23618090452261306\n",
      "0.23095823095823095\n",
      "-0.013230328899216482\n",
      "[[484 161]\n",
      " [152  47]]\n"
     ]
    }
   ],
   "source": [
    "getResult(\"openstack\",2,\"ironic\",data_ironic,-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145bf1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "getResult(\"qt\",2,\"base\",data_base,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo-env",
   "language": "python",
   "name": "neo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
